\chapter{Evaluation and Results}
\label{chap:res}

This chapter presents the implementation artifacts and analyzes the library by comparing
solvers implemented with and without use of the library by their code size and
complexity, performance, and subjective implementation difficulty. Cyclomatic complexity
\cite{ebert2016cyclomatic} is used as a metric of code complexity,
despite the controversy around it, as it is extensively used in the industry
\cite{ebert2016cyclomatic}.

The last section compares the generic features supported by the presented library and
other libraries that implement branch-and-bound or backtracking.

\section{Implementation artifacts}

The source code of the library and all of the examples (solvers) is electronically
available at \url{https://github.com/kolayne/Branch-and-Bound-templates}. The library
is available for download and installation as a Rust crate at
\url{https://crates.io/crates/branch-and-bound}.
% TODO: it is available under the terms of the MIT license.
The up-to-date documentation of the latest version of the library is electronically
available at \url{https://docs.rs/branch-and-bound/latest/branch_and_bound/}.
The documentation of the current version can be found in the appendix chapter \ref{appex:libdoc}.

\section{Solvers comparison}

This section compares library-based and native solvers of CNF SAT and the Knapsack problem.
For CNF SAT, I first implemented a native solver, then a library-based
solver. For the Knapsack problem, I first implemented a library-based solver, then a
native solver.

\subsection{Code and implementation complexity}

The source code of solvers for both problem was organized in three modules:
the main file of the library-based solver, the main file of the native solver, and a file
with core code implementing types and basic methods that are used by both solvers.

The amount of code per file for solvers of both problems can be found in table
\ref{tab:loc_solvers} (empty lines, comments, and code related to input parsing
and unit testing are excluded).

\begin{table}[h!]
 \centering
 \caption[Lines of code per problem solver implementation (CNF SAT and Knapsack)]
    {Lines of code (LoC) per implementation file}
 \label{tab:loc_solvers}

 \begin{tabular}{|c||c|c|c|}
  \hline
  & library-based solver code & native solver code & core code \\
  \hline
  CNF SAT & 77 & 66 & 133 \\
  \hline
  Knapsack & 32 & 44 & 136 \\
  \hline
 \end{tabular}
\end{table}

The values of the cyclomatic complexity metric \cite{ebert2016cyclomatic} for functions
implemented in the solvers can be found in tables \ref{tab:CC_cnfsat} and
\ref{tab:CC_knapsack} (functions related to input parsing and unit testing are excluded)
\footnote{Cyclomatic complexity was calculated automatically using \emph{lizard}:
\url{https://github.com/terryyin/lizard}}.

\begin{table}
 \centering
 \caption{Cyclomatic complexity per function of the CNF SAT solvers}
 \label{tab:CC_cnfsat}

 \begin{tabular}{|c|c|c|c|}
  \hline
  filename & function name & LoC & CC \\
  \hline\hline
  \multirow{4}*{examples/dpll.rs} & \texttt{branch\_or\_evaluate} & 50 & 6 \\
  & \texttt{solve} & 10 & 1 \\
  & \texttt{main} & 3 & 1 \\
  & \texttt{bound} & 1 & 1 \\
  \hline
  \multirow{3}*{examples/dpll-without-library.rs} & \texttt{solve\_dfs} & 52 & 10 \\
  & \texttt{solve} & 8 & 2 \\
  & \texttt{main} & 3 & 1 \\
  \hline
  \multirow{5}*{examples/dpll\_core/mod.rs} & \texttt{eval} & 20 & 8 \\
  & \texttt{examples\_main} & 18 & 5 \\
  & \texttt{assert\_solves} & 9 & 3 \\
  \hline
 \end{tabular}
\end{table}

\begin{table}
 \centering
 \caption{Cyclomatic complexity per function of the Knapsack problem solvers}
 \label{tab:CC_knapsack}

 \begin{tabular}{|c|c|c|c|}
  \hline
  filename & function/method name & LoC & CC \\
  \hline\hline
  \multirow{4}*{examples/knapsack.rs} & \texttt{branch\_or\_evaluate} & 15 & 3 \\
  & \texttt{bound} & 3 & 1 \\
  & \texttt{solve} & 3 & 1 \\
  & \texttt{main} & 3 & 1 \\
  \hline
  \multirow{3}*{examples/knapsack-without-library.rs} & \texttt{solve} & 29 & 7 \\
  & \texttt{best\_candidate} & 7 & 2 \\
  & \texttt{main} & 3 & 1 \\
  \hline
  \multirow{11}*{examples/knapsack\_core/mod.rs} & \texttt{pop\_too\_heavy} & 9 & 3 \\
  & \texttt{bound} & 13 & 3 \\
  & \texttt{examples\_main} & 12 & 2 \\
  & \texttt{new} & 13 & 1 \\
  & \texttt{include\_next} & 8 & 1 \\
  & \texttt{future\_items} & 6 & 1 \\
  & \texttt{drop\_next} & 5 & 1 \\
  & \texttt{have\_items} & 3 & 1 \\
  & \texttt{capacity\_left} & 3 & 1 \\
  & \texttt{collected\_val} & 3 & 1 \\
  & \texttt{into\_items} & 3 & 1 \\
  \hline
 \end{tabular}
\end{table}

Overall, the library-based implementation of the CNF SAT solver (excluding core code)
was $\approx 16.7\%$ longer (77 lines compared to 66 lines in the native implementation),
while the library-based implementation of the knapsack problem solver (excluding
core code) was $37.5\%$ shorter (32 lines compared to 44 lines in the native
implementation). So, there is no clear relation betwen the usage of the library and the
code size.

As for the cyclomatic complexity metric, the value for the most complex function
is lower for the library-based implementation in both cases:
$\approx 66.7\%$ lower for the CNF SAT problem
(6 for \texttt{branch\_or\_evaluate} from the library-based implementation and
10 for \texttt{solve\_dfs} from the native implementation)
and $\approx 133.3\%$ lower for the knapsack problem implementation
(3 for \texttt{branch\_or\_evaluate} from the library-based implementation and
7 for \texttt{solve} from the native implementation).
This may suggest that solvers designed and implemented for work with the library
generally have simpler code.

Note: code complexity is affected by aspects of the way branch-and-bound method
is implemented, such as evaluation strategy: in the knapsack examples, both eager evaluation
and lazy evaluation checks are performed, resulting in the increased cyclomatic complexity
in the native solver (for the library-based solver, evaluation strategy is encapsulated
by the library).

Finally, subjectively, there was no significant difference in the difficulty of
implementation of solvers for CNF SAT and the Knapsack problem with and without the library.
It is still required for the programmer to understand the problem they are implementing,
and, although there is no need to explicitly program the subproblems traversal and candidates
filtering, implementing the interface required by the library for a problem requires thinking
of the problem from an unusual perspective, which takes additional effort.

\subsection{Performance}

To test solvers and evaluate their performance, the following datasets were used:
a CNF SAT input/output dataset available at
\url{https://people.sc.fsu.edu/~jburkardt/data/cnf/cnf.html}
and a Knapsack problem input/output dataset available at
\url{https://people.sc.fsu.edu/~jburkardt/datasets/knapsack_01/knapsack_01.html}
\footnote{Both datasets are distributed under the terms of GNU LGPL version 3}.
Comprehensive testing is not claimed, but the datasets are linked for the purpose of
reproducibility.

For performance evaluation, I selected 3 CNF SAT tests that take longest for
both CNF SAT solver implementations and 1 Knapsack test that takes longest for both
Knapsack problem solver implementations.
I ran each implementation 15 times on every test, excluded two shortest and two longest
runs, and calculated the average time it took the implementations to solve the problems.

The results are presented in tables \ref{tab:perf_cnfsat} and \ref{tab:perf_knapsack}.
The values are given in the form $A\pm B$, where $A$ is the average run time,
and $B$ is a value such that all the considered run times (except for the four outliers) are in
the range $[A - B; A + B]$.

\begin{table}[h]
 \centering
 \caption{Average time per run for the CNF SAT solvers}
 \label{tab:perf_cnfsat}

 \begin{tabular}{|ccc|}
  \hline
  Test name & library-based solver time, s & native solver time, s \\
  \hline
  dubois\_20.cnf & $\approx 12.300\pm 0.297$ & $\approx 11.512\pm 0.288$ \\
  dubois\_21.cnf & $\approx 25.702\pm 0.456$ & $\approx 24.040\pm 0.329$ \\
  dubois\_22.cnf & $\approx 52.609\pm 0.329$ & $\approx 50.715\pm 0.633$ \\
  \hline
 \end{tabular}
\end{table}

\begin{table}[h]
 \centering
 \caption{Average time per run for the Knapsack problem solvers}
 \label{tab:perf_knapsack}

 \begin{tabular}{|ccc|}
  \hline
  Test name & library-based solver time, s & native solver time, s \\
  \hline
  Test 8 & $\approx 0.379\pm 0.006$ & $\approx 0.293\pm 0.009$ \\
  \hline
 \end{tabular}
\end{table}

Thus, the use of the library has a negative influence on the performance of solvers:
from $\approx 3.7\%$ slower (in dubois\_22.cnf) to $\approx 6.9\%$ slower (in dubois\_21.cnf)
for CNF SAT problem samples, and $\approx 29.4\%$ slower for the Knapsack problem sample.

The significant difference between performance gaps of the CNF SAT solvers and the Knapsack solvers
may be due to either the library's inefficiency with the branch-and-bound method, or a human
error in the library-based knapsack solver implementation, or a testing problem
(the test run time is so short that the abstraction overhead might be more noticable).

The relative execution time difference between CNF SAT solvers is smallest for the
dubois\_22 test, which takes longest to solve for both solvers. While this may
suggest that for more complex inputs the relative library-incurred overhead is lower,
there are not enough data to conclude that. First, the relative execution
time difference is higher for the dubois\_21 (larger) test than for the dubois\_20 (smaller)
test, which may or may not be due to individual test features. Second, a trend in a single
library-based implementation is not sufficient to draw a conclusion about the library.

Note: because performance was not a primary focus during the development of the library,
some decisions during its implementation were made in the favor of easy usage rather than
high performance. The performance of the library can be improved by revisiting these
decisions, as well as by profiling and optimizing library code.

\section{Comparison against other generic solvers}

\label{sec:feature_cmp}

This section analyzes the library feature set by comparing it to other libraries for
backtracking and branch-and-bound. Libraries used in the comparison are from works
cited in chapter \ref{chap:lr}, as well as open-source libraries that were found
with the 'backtracking' and 'branch bound' search queries on either GitHub
or crates.io.

The table below presents the assesment of the libraries and lists all the search features
that were implemented in at least one of the libraries. The table contains the following
columns:
\begin{itemize}
 \item \textbf{NAME} - the library name (e.g., repository name on GitHub
      or crate name on crates.io) and/or the paper where it is presented;
 \item \textbf{PL} - the programming language of the library implementation;
 \item \textbf{DOC} - whether documentation is available, either together with the source code
      or on a separate resource ('Paper only' indicates that the library interface
      is only documented in the paper);
 \item \textbf{EXAMP} - whether code examples of the library usage are available;
 \item \textbf{ORDER} - supported traverse orders, out of:
      Dfs, Bfs, Greedy (aka best-first search), Custom order;
 \item \textbf{OPAQ} - whether opaque types are supported for storing state, i.e., the user
      may use arbitrary types to represent subproblems and solutions, and the library does
      not depend on a particular state structure;
 \item \textbf{EXIT} - whether local exit and global exit \cite{narkawicz2013formalnasa}
      are supported
      ('Partial' means that only one of the two is supported);
 \item \textbf{EARLY TERM.} - whether early termination with greedy search or custom-order
      search is supported;
 \item \textbf{LAZ/EAG} - whether the choice between lazy and eager evaluation strategies
      is supported;
 \item \textbf{TOLER} - whether tolerance is supported, i.e., the solver may return an
      imprecise solution with an error value not exceeding the given tolerance;
 \item \textbf{LASER} - whether for depth-first search the library allows for one-at-a-time
      subproblem generation instead of requiring to generate all subproblems at once
      (this is referred to as the \emph{laser} search in \cite{johnson1988modular});
      this feature is discussed in more detail in section \ref{sub:design_improvements}.
 \item \textbf{EXTE} - whether library extension is supported, e.g., customizing the
      search process, using an alternative container for subproblem storage, etc.
\end{itemize}

The value '?' in the table indicates that the answer is not clear from the available
documentation, examples, or the library description. The value 'Extendable' means that the
library allows for the feature support via an extension.

\renewcommand{\thempfootnote}{\arabic{mpfootnote}}
\begin{sidewaystable}
\caption{Feature set comparison for branch-and-bound and backtracking libraries}
\label{tab:feature_cmp}

\begin{adjustbox}{width=\textwidth}
\begin{threeparttable}
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
  \hline
  \textbf{NAME} & \textbf{PL} & \textbf{DOC} & \textbf{EXAMP} & \textbf{ORDER} & \textbf{OPAQ} & \textbf{EXIT} & \textbf{EARLY TERM.} & \textbf{LAZ/EAG} & \textbf{TOLER} & \textbf{LASER} & \textbf{EXTE} \\
  \hline
  branch\_bound\_method \footnote{\url{https://crates.io/crates/branch\_bound\_method}} & Rust & \xmark & \xmark & ? & ? & ? & ? & ? & ? & ? & ? \\
  \hline
  branch-and-bound \footnote{\url{https://github.com/UIUC-optimization/branch-and-bound}} & C++ & \xmark & \xmark & Dfs, Bfs, Greedy & ? & ? & ? & ? & ? & ? & ? \\
  \hline
  bnb \footnote{\url{https://crates.io/crates/bnb}} & Rust & \cmark & \xmark & Dfs, Bfs, Greedy & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark \\
  \hline
  backtrack* \footnote{\url{https://crates.io/crates/backtrack}} & Rust & \cmark & \xmark & Bfs & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark \\
  \hline
  backtracking* \footnote{\url{https://crates.io/crates/backtracking}} & Rust & \cmark & \cmark & Bfs & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark \\
  \hline
  BranchAndBound.jl \footnote{\url{https://github.com/kibaekkim/BranchAndBound.jl}} & Julia & \xmark & \cmark & Greedy & \cmark & Partial & \xmark & \xmark & \xmark & \xmark & \xmark \\
  \hline
  pybnb \footnote{\url{https://github.com/ghackebeil/pybnb}} & Python & \cmark & \cmark & Dfs, Bfs, Greedy, Custom & \cmark & \xmark & \xmark & \xmark & \cmark & \xmark & \xmark \\
  \hline
  Kodiak \footnote{\url{https://github.com/nasa/Kodiak}} \cite{narkawicz2013formalnasa} & C++ & Paper only & \cmark & Dfs & \cmark & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark \\
  \hline
  ddbnb \footnote{\url{https://github.com/distcomp/ddbnb}} \cite{voloshinov2017implementation} & \makecell{Python,\\C++,\\Erlang} & \xmark & \xmark & N/A\footnote{Distributed implementation} & ? & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark \\
  \hline
  (Finkel, Manber) \cite{finkel1987distrib} & Modula & \cmark & \cmark & N/A\footnote{Distributed implementation} & ? & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark \\
  \hline
  (Johnson) \cite{johnson1988modular} & ? & Paper only & \xmark & Dfs, Bfs & ?  & ? & ? & ? & ? & \cmark & \xmark \\
  \hline
  Presented work & Rust & \cmark & \cmark & Dfs, Bfs, Greedy, Custom & \cmark & Extendable & \cmark & Extendable & \xmark & \xmark & \cmark \\
  \hline
  \end{tabular}

  \begin{tablenotes}
    %\small
    \item * Only binary objective function is supported.
  \end{tablenotes}
\end{threeparttable}
\end{adjustbox}
\end{sidewaystable}
