\chapter{Evaluation and Results}
\label{chap:res}

This chapter presents the implementation artifacts and analyzes the library by comparing
solvers implemented with and without use of the library by their code size and
complexity, performance, and subjective implementation difficulty. Cyclomatic complexity
\cite{ebert2016cyclomatic} is used as a metric of code complexity,
despite the controversy around it, as it is extensively used in the industry
\cite{ebert2016cyclomatic}.

\section{Implementation artifacts}

The source code of the library and all of the examples (solvers) is electronically
available at \url{https://github.com/kolayne/Branch-and-Bound-templates}. The library
is available for download and installation as a Rust crate at
\url{https://crates.io/crates/branch-and-bound}.
% TODO: it is available under the terms of the MIT license.
The up-to-date documentation of the latest version of the library is electronically
available at \url{https://docs.rs/branch-and-bound/latest/branch_and_bound/}.
The documentation of the current version can be found in the appendix chapter \ref{appex:libdoc}.

\section{Solvers comparison}

This section compares library-based and native solvers of CNF SAT and the Knapsack problem.
For CNF SAT, I first implemented a native solver, then a library-based
solver. For the Knapsack problem, I first implemented a library-based solver, then a
native solver.

\subsection{Code and implementation complexity}

The source code of solvers for both problem was organized in three modules:
the main file of the library-based solver, the main file of the native solver, and a file
with core code implementing types and basic methods that are used by both solvers.

The amount of code per file for solvers of both problems can be found in table
\ref{tab:loc_solvers} (empty lines, comments, and code related to input parsing
and unit testing are excluded).

\begin{table}[h!]
 \centering
 \caption[Lines of code per problem solver implementation (CNF SAT and Knapsack)]
    {Lines of code (LoC) per implementation file}
 \label{tab:loc_solvers}

 \begin{tabular}{|c||c|c|c|}
  \hline
  & library-based solver code & native solver code & core code \\
  \hline
  CNF SAT & 77 & 66 & 133 \\
  \hline
  Knapsack & 32 & 44 & 136 \\
  \hline
 \end{tabular}
\end{table}

The values of the cyclomatic complexity metric \cite{ebert2016cyclomatic} for functions
implemented in the solvers can be found in tables \ref{tab:CC_cnfsat} and
\ref{tab:CC_knapsack} (functions related to input parsing and unit testing are excluded)
\footnote{Cyclomatic complexity was calculated automatically using \emph{lizard}:
\url{https://github.com/terryyin/lizard}}.

\begin{table}
 \centering
 \caption{Cyclomatic complexity per function of the CNF SAT solvers}
 \label{tab:CC_cnfsat}

 \begin{tabular}{|c|c|c|c|}
  \hline
  filename & function name & LoC & CC \\
  \hline\hline
  \multirow{4}*{examples/dpll.rs} & \texttt{branch\_or\_evaluate} & 50 & 6 \\
  & \texttt{solve} & 10 & 1 \\
  & \texttt{main} & 3 & 1 \\
  & \texttt{bound} & 1 & 1 \\
  \hline
  \multirow{3}*{examples/dpll-without-library.rs} & \texttt{solve\_dfs} & 52 & 10 \\
  & \texttt{solve} & 8 & 2 \\
  & \texttt{main} & 3 & 1 \\
  \hline
  \multirow{5}*{examples/dpll\_core/mod.rs} & \texttt{eval} & 20 & 8 \\
  & \texttt{examples\_main} & 18 & 5 \\
  & \texttt{assert\_solves} & 9 & 3 \\
  \hline
 \end{tabular}
\end{table}

\begin{table}
 \centering
 \caption{Cyclomatic complexity per function of the Knapsack problem solvers}
 \label{tab:CC_knapsack}

 \begin{tabular}{|c|c|c|c|}
  \hline
  filename & function/method name & LoC & CC \\
  \hline\hline
  \multirow{4}*{examples/knapsack.rs} & \texttt{branch\_or\_evaluate} & 15 & 3 \\
  & \texttt{bound} & 3 & 1 \\
  & \texttt{solve} & 3 & 1 \\
  & \texttt{main} & 3 & 1 \\
  \hline
  \multirow{3}*{examples/knapsack-without-library.rs} & \texttt{solve} & 29 & 7 \\
  & \texttt{best\_candidate} & 7 & 2 \\
  & \texttt{main} & 3 & 1 \\
  \hline
  \multirow{11}*{examples/knapsack\_core/mod.rs} & \texttt{pop\_too\_heavy} & 9 & 3 \\
  & \texttt{bound} & 13 & 3 \\
  & \texttt{examples\_main} & 12 & 2 \\
  & \texttt{new} & 13 & 1 \\
  & \texttt{include\_next} & 8 & 1 \\
  & \texttt{future\_items} & 6 & 1 \\
  & \texttt{drop\_next} & 5 & 1 \\
  & \texttt{have\_items} & 3 & 1 \\
  & \texttt{capacity\_left} & 3 & 1 \\
  & \texttt{collected\_val} & 3 & 1 \\
  & \texttt{into\_items} & 3 & 1 \\
  \hline
 \end{tabular}
\end{table}

Overall, the library-based implementation of the CNF SAT solver (excluding core code)
was $\approx 16.7\%$ longer (77 lines compared to 66 lines in the native implementation),
while the library-based implementation of the knapsack problem solver (excluding
core code) was $37.5\%$ shorter (32 lines compared to 44 lines in the native
implementation). So, there is no clear relation betwen the usage of the library and the
code size.

As for the cyclomatic complexity metric, the value for the most complex function
is lower for the library-based implementation in both cases:
$\approx 66.7\%$ lower for the CNF SAT problem
(6 for \texttt{branch\_or\_evaluate} from the library-based implementation and
10 for \texttt{solve\_dfs} from the native implementation)
and $\approx 133.3\%$ lower for the knapsack problem implementation
(3 for \texttt{branch\_or\_evaluate} from the library-based implementation and
7 for \texttt{solve} from the native implementation).
This may suggest that solvers designed and implemented for work with the library
generally have simpler code.

Note: code complexity is affected by aspects of the way branch-and-bound method
is implemented, such as evaluation strategy: in the knapsack examples, both eager evaluation
and lazy evaluation checks are performed, resulting in the increased cyclomatic complexity
in the native solver (for the library-based solver, evaluation strategy is encapsulated
by the library).

Finally, subjectively, there was no significant difference in the difficulty of
implementation of solvers for CNF SAT and the Knapsack problem with and without the library.
It is still required for the programmer to understand the problem they are implementing,
and, although there is no need to explicitly program the subproblems traversal and candidates
filtering, implementing the interface required by the library for a problem requires thinking
of the problem from an unusual perspective, which takes additional effort.

\subsection{Performance}

To test solvers and evaluate their performance, the following datasets were used:
a CNF SAT input/output dataset available at
\url{https://people.sc.fsu.edu/~jburkardt/data/cnf/cnf.html}
and a Knapsack problem input/output dataset available at
\url{https://people.sc.fsu.edu/~jburkardt/datasets/knapsack_01/knapsack_01.html}
\footnote{Both datasets are distributed under the terms of GNU LGPL version 3}.
Comprehensive testing is not claimed, but the datasets are linked for the purpose of
reproducibility.

For performance evaluation, I selected 3 CNF SAT tests that take longest for
both CNF SAT solver implementations and 1 Knapsack test that takes longest for both
Knapsack problem solver implementations.
I ran each implementation 15 times on every test, excluded two shortest and two longest
runs, and calculated the average time it took the implementations to solve the problems.

The results are presented in tables \ref{tab:perf_cnfsat} and \ref{tab:perf_knapsack}.
The values are given in the form $A\pm B$, where $A$ is the average run time,
and $B$ is a value such that all the considered run times (except for the four outliers) are in
the range $[A - B; A + B]$.

\begin{table}[h]
 \centering
 \caption{Average time per run for the CNF SAT solvers}
 \label{tab:perf_cnfsat}

 \begin{tabular}{|ccc|}
  \hline
  Test name & library-based solver time, s & native solver time, s \\
  \hline
  dubois\_20.cnf & $\approx 12.300\pm 0.297$ & $\approx 11.512\pm 0.288$ \\
  dubois\_21.cnf & $\approx 25.702\pm 0.456$ & $\approx 24.040\pm 0.329$ \\
  dubois\_22.cnf & $\approx 52.609\pm 0.329$ & $\approx 50.715\pm 0.633$ \\
  \hline
 \end{tabular}
\end{table}

\begin{table}[h]
 \centering
 \caption{Average time per run for the Knapsack problem solvers}
 \label{tab:perf_knapsack}

 \begin{tabular}{|ccc|}
  \hline
  Test name & library-based solver time, s & native solver time, s \\
  \hline
  Test 8 & $\approx 0.379\pm 0.006$ & $\approx 0.293\pm 0.009$ \\
  \hline
 \end{tabular}
\end{table}

Thus, the use of the library has a negative influence on the performance of solvers:
from $\approx 3.7\%$ slower (in dubois\_22.cnf) to $\approx 6.9\%$ slower (in dubois\_21.cnf)
for CNF SAT problem samples, and $\approx 29.4\%$ slower for the Knapsack problem sample.

The significant difference between performance gaps of the CNF SAT solvers and the Knapsack solvers
may be due to either the library's inefficiency with the branch-and-bound method, or a human
error in the library-based knapsack solver implementation, or a testing problem
(the test run time is so short that the abstraction overhead might be more noticable).

The relative execution time difference between CNF SAT solvers is smallest for the
dubois\_22 test, which takes longest to solve for both solvers. While this may
suggest that for more complex inputs the relative library-incurred overhead is lower,
there are not enough data to conclude that. First, the relative execution
time difference is higher for the dubois\_21 (larger) test than for the dubois\_20 (smaller)
test, which may or may not be due to individual test features. Second, a trend in a single
library-based implementation is not sufficient to draw a conclusion about the library.

Note: because performance was not a primary focus during the development of the library,
some decisions during its implementation were made in the favor of easy usage rather than
high performance. The performance of the library can be improved by revisiting these
decisions, as well as by profiling and optimizing library code.
